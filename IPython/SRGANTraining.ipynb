{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "from math import log10\n",
    "\n",
    "import pandas as pd\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.utils as utils\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from src.configs import config\n",
    "from src.modules.srgan.data_utils import HPATrainDatasetFromFolder, HPAValDatasetFromFolder, display_transform\n",
    "from src.modules.srgan.loss import GeneratorLoss\n",
    "from src.modules.srgan.model import Generator, Discriminator\n",
    "import src.modules.srgan.ssim as pytorch_ssim\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_SRGAN(train_data_dir, valid_data_dir, process_data_dir,\n",
    "                crop_size=88, upscale_factor=4, num_epochs=10):\n",
    "    \n",
    "    # Load test and train sets\n",
    "    train_set = HPATrainDatasetFromFolder(train_data_dir, crop_size=crop_size, upscale_factor=upscale_factor)\n",
    "    val_set = HPAValDatasetFromFolder(valid_data_dir, upscale_factor=upscale_factor)\n",
    "    train_loader = DataLoader(dataset=train_set, num_workers=24, batch_size=48, shuffle=True)\n",
    "    val_loader = DataLoader(dataset=val_set, num_workers=8, batch_size=1, shuffle=False)\n",
    "\n",
    "    # Initialize the networks\n",
    "    netG = Generator(upscale_factor)\n",
    "    print('# generator parameters:', sum(param.numel() for param in netG.parameters()))\n",
    "    netD = Discriminator()\n",
    "    print('# discriminator parameters:', sum(param.numel() for param in netD.parameters()))\n",
    "\n",
    "    generator_criterion = GeneratorLoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        netG.cuda()\n",
    "        netD.cuda()\n",
    "        generator_criterion.cuda()\n",
    "\n",
    "    optimizerG = optim.Adam(netG.parameters())\n",
    "    optimizerD = optim.Adam(netD.parameters())\n",
    "    \n",
    "    results = {'d_loss': [], 'g_loss': [], 'd_score': [], 'g_score': [], 'psnr': [], 'ssim': []}\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        train_bar = tqdm_notebook(train_loader)\n",
    "        running_results = {'batch_sizes': 0, 'd_loss': 0, 'g_loss': 0, 'd_score': 0, 'g_score': 0}\n",
    "\n",
    "        netG.train()\n",
    "        netD.train()\n",
    "        for data, target in train_bar:\n",
    "            g_update_first = True\n",
    "            batch_size = data.size(0)\n",
    "            running_results['batch_sizes'] += batch_size\n",
    "\n",
    "            ############################\n",
    "            # (1) Update D network: maximize D(x)-1-D(G(z))\n",
    "            ###########################\n",
    "            real_img = Variable(target)\n",
    "            if torch.cuda.is_available():\n",
    "                real_img = real_img.cuda()\n",
    "            z = Variable(data)\n",
    "            if torch.cuda.is_available():\n",
    "                z = z.cuda()\n",
    "            fake_img = netG(z)\n",
    "\n",
    "            netD.zero_grad()\n",
    "            real_out = netD(real_img).mean()\n",
    "            fake_out = netD(fake_img).mean()\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            d_loss.backward(retain_graph=True)\n",
    "            optimizerD.step()\n",
    "\n",
    "            ############################\n",
    "            # (2) Update G network: minimize 1-D(G(z)) + Perception Loss + Image Loss + TV Loss\n",
    "            ###########################\n",
    "            netG.zero_grad()\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            g_loss.backward()\n",
    "\n",
    "            optimizerG.step()\n",
    "            fake_img = netG(z)\n",
    "            fake_out = netD(fake_img).mean()\n",
    "\n",
    "            g_loss = generator_criterion(fake_out, fake_img, real_img)\n",
    "            running_results['g_loss'] += g_loss.data.item() * batch_size\n",
    "            d_loss = 1 - real_out + fake_out\n",
    "            running_results['d_loss'] += d_loss.data.item() * batch_size\n",
    "            running_results['d_score'] += real_out.data.item() * batch_size\n",
    "            running_results['g_score'] += fake_out.data.item() * batch_size\n",
    "\n",
    "            train_bar.set_description(desc='[%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f' % (\n",
    "                epoch, num_epochs, running_results['d_loss'] / running_results['batch_sizes'],\n",
    "                running_results['g_loss'] / running_results['batch_sizes'],\n",
    "                running_results['d_score'] / running_results['batch_sizes'],\n",
    "                running_results['g_score'] / running_results['batch_sizes']))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            netG.eval()\n",
    "            out_path = process_data_dir + '/training_results/SRF_' + str(upscale_factor) + '/'\n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "                \n",
    "            val_bar = tqdm_notebook(val_loader)\n",
    "            valing_results = {'mse': 0, 'ssims': 0, 'psnr': 0, 'ssim': 0, 'batch_sizes': 0}\n",
    "            val_images = []\n",
    "            \n",
    "            for val_lr, val_hr_restore, val_hr in val_bar:\n",
    "                batch_size = val_lr.size(0)\n",
    "                valing_results['batch_sizes'] += batch_size\n",
    "                lr = Variable(val_lr)\n",
    "                hr = Variable(val_hr)\n",
    "                if torch.cuda.is_available():\n",
    "                    lr = lr.cuda()\n",
    "                    hr = hr.cuda()\n",
    "                sr = netG(lr)\n",
    "\n",
    "                batch_mse = ((sr - hr) ** 2).data.mean()\n",
    "                valing_results['mse'] += batch_mse * batch_size\n",
    "                batch_ssim = pytorch_ssim.ssim(sr, hr).data.item()\n",
    "                valing_results['ssims'] += batch_ssim * batch_size\n",
    "                valing_results['psnr'] = 10 * log10(1 / (valing_results['mse'] / valing_results['batch_sizes']))\n",
    "                valing_results['ssim'] = valing_results['ssims'] / valing_results['batch_sizes']\n",
    "                val_bar.set_description(\n",
    "                    desc='[converting LR images to SR images] PSNR: %.4f dB SSIM: %.4f' % (\n",
    "                        valing_results['psnr'], valing_results['ssim']))\n",
    "\n",
    "                val_images.extend(\n",
    "                    [display_transform()(val_hr_restore.squeeze(0)), display_transform()(hr.data.cpu().squeeze(0)),\n",
    "                     display_transform()(sr.data.cpu().squeeze(0))])\n",
    "            val_images = torch.stack(val_images)\n",
    "            val_images = torch.chunk(val_images, val_images.size(0) // 15)\n",
    "            val_save_bar = tqdm_notebook(val_images, desc='[saving training results]')\n",
    "            index = 1\n",
    "            for image in val_save_bar:\n",
    "                image = utils.make_grid(image, nrow=3, padding=5)\n",
    "                utils.save_image(image, out_path + 'epoch_%d_index_%d.png' % (epoch, index), padding=5)\n",
    "                index += 1\n",
    "\n",
    "        # save model parameters\n",
    "        epochs_path = process_data_dir + '/epochs'\n",
    "        if not os.path.exists(epochs_path):\n",
    "            os.makedirs(epochs_path)\n",
    "            \n",
    "        torch.save(netG.state_dict(), epochs_path +'/netG_epoch_%d_%d.pth' % (upscale_factor, epoch))\n",
    "        torch.save(netD.state_dict(), epochs_path + '/netD_epoch_%d_%d.pth' % (upscale_factor, epoch))\n",
    "        # save loss\\scores\\psnr\\ssim\n",
    "        results['d_loss'].append(running_results['d_loss'] / running_results['batch_sizes'])\n",
    "        results['g_loss'].append(running_results['g_loss'] / running_results['batch_sizes'])\n",
    "        results['d_score'].append(running_results['d_score'] / running_results['batch_sizes'])\n",
    "        results['g_score'].append(running_results['g_score'] / running_results['batch_sizes'])\n",
    "        results['psnr'].append(valing_results['psnr'])\n",
    "        results['ssim'].append(valing_results['ssim'])\n",
    "\n",
    "        if epoch % 1 == 0 and epoch != 0:\n",
    "            out_path = process_data_dir + '/statistics/'\n",
    "            if not os.path.exists(out_path):\n",
    "                os.makedirs(out_path)\n",
    "            \n",
    "            data_frame = pd.DataFrame(\n",
    "                data={'Loss_D': results['d_loss'], 'Loss_G': results['g_loss'], 'Score_D': results['d_score'],\n",
    "                      'Score_G': results['g_score'], 'PSNR': results['psnr'], 'SSIM': results['ssim']},\n",
    "                index=range(1, epoch + 1))\n",
    "            data_frame.to_csv(out_path + 'srf_' + str(upscale_factor) + '_train_results.csv', index_label='Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_DIR = '../data/train'\n",
    "VAL_DATA_DIR = '../data/valid'\n",
    "PROCESS_DATA_DIR = '../srgan_training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# generator parameters: 713481\n",
      "# discriminator parameters: 5214273\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c80fd39efa84ea384834b205aabbe62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=644), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1339: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:1350: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8be68e8e0be943a29709ac2dc0ac570f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29bb5745022a416cbed30ea7d0d639f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[saving training results]', max=5, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e7e314875584c899fcb22e44fdc438f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=644), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9aee8f5a0f05429a9119621ed6775515",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681f6f59bf5e4fe088310f2e3b486904",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[saving training results]', max=5, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5fae84cfde147f38781ccdf2335a6de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=644), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba40900c7b93459e86a718a0490de8a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=25), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afab1404cd354e7e85e8a1df945f5385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='[saving training results]', max=5, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30b9cc29b3d6451fbba2d0913465abb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=644), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_SRGAN(TRAIN_DATA_DIR, VAL_DATA_DIR, PROCESS_DATA_DIR, num_epochs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
